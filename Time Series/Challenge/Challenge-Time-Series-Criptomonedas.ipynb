{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHALLENGE: Time Series Analysis: Cryptocurrencies\n",
    "\n",
    "### This exercise has been proposed and created entirely by the AI ​​Saturdays Euskadi team, taking adaptations from other sources\n",
    "\n",
    "A common case of Time Series analysis work is that referring to different stock market assets. In this exercise, we are going to work with a dataset that contains a series of values ​​of a specific cryptocurrency (<a href=\"https://en.wikipedia.org/wiki/Ripple_(payment_protocol)\">XRP</a >) . XRP is one of the most popular cryptocurrencies that recently got the second position in terms of market capitalization and also has great potential in the future, in terms of speed of payment and transactions. In fact, XRP is often referred to as the “bankers currency” as institutions and banks have partnered with Ripple for their various transaction enhancement procedures.\n",
    "\n",
    "<p style=\"text-align: allow\">The historical data of the coin has been taken from the web by webscraping using the library <a href=\"https://www.crummy.com/software/BeautifulSoup/\"> Beautiful Soup</a> and have been saved in <em>CSV</em> format.\n",
    "\n",
    "<p style=\"text-align: allow\">The <b>\"XRP_price\"</b> dataset is a multivariate time series dataset describing the historical price of the coin since its near debut as a currency. The data goes from August 2013 to December 2018, with daily observations of the different quantities. It is a multivariate series composed of 3 variables (in addition to the date and time); son:</p>\n",
    "\n",
    "<ul>\n",
    "<li><strong>Close XRP</strong>: The daily closing price of the cryptocurrency (USD).</li>\n",
    "<li><strong>XRP Volume</strong>: The total daily volume of the coin (USD).</li>\n",
    "<li><strong>Market Cap</strong>: The total daily market capitalization of the currency (USD).</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import regular libraries \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from SciPy packages\n",
    "from statsmodels.tsa.stattools import adfuller # adfuller test\n",
    "from statsmodels.graphics.tsaplots import plot_acf # autocorellation plot\n",
    "from statsmodels.graphics.tsaplots import plot_pacf # partial autocorellation plot\n",
    "\n",
    "# math function\n",
    "from math import sqrt\n",
    "\n",
    "# evaluation metrics\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# statistics models\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# libraries to filter warnings in some algorithms\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Dataset\n",
    "\n",
    "<p style=\"text-align: justify\">After importing the different libraries that will be used in the code, we can use the <em>read_csv()</em> function to load the data and infer the first column containing the date data as an index.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The dataset contains some <em>NaN</em> at the start of the historical data for the volume of the coin. These values are set to be 0 and won't influence the current study. We also make sure that the dataset is composed of floating point values of the same type.</p>\n",
    "\n",
    "\n",
    "<p style=\"text-align: justify\">We can perform some operations such as calculating the number of days per year in our dataset. This can easily be achieved using the <a href=\"https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.resample.html\">resample() function</a> on the pandas DataFrame. Calling this function with the argument &#8216;<em>Y</em>&#8216; allows the loaded data indexed by date-time to be grouped by year (<a href=\"http://pandas.pydata.org/pandas-docs/stable/timeseries.html#offset-aliases\">see all offset aliases</a>). We can then count the number of days for each year. Finally ,different properties of the dataset can be printed to check the statistics or shape for instance.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write name of coin for study (XRP, BTC)\n",
    "\n",
    "dataset = pd.read_csv(, header=0, infer_datetime_format=True, parse_dates=['Date'], index_col=['Date'])\n",
    "\n",
    "# fill all NaN values with some particular value\n",
    "\n",
    "# make dataset numeric --> astype\n",
    "\n",
    "# look at the values of the dataset\n",
    "values = dataset.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting number of days per year\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a series from the values of the dataframe\n",
    "series = pd.Series(values[:,0])\n",
    "series.index = dataset.index\n",
    "series.index.name = 'date'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Data Visualization\n",
    "\n",
    "<p style=\"text-align: justify\">Then a series of curves can be plotted to visualize the dataset and understand if the data answers to a particular behaviour. \n",
    "    \n",
    "The first type of plot visualization we see here is most common one: the line plot. It shows the evolution of the price with time. It can be seen that the price increased substantially since 2017, which is the year where most people became aware of cryptocurrencies. To have a better insight of the price evolution, we can plot the time series after taking its **logarithm transformation**. \n",
    "    \n",
    "A <em>log</em> transformation is often used in order to turn a time series <a href=\"https://en.wikipedia.org/wiki/Stationary_process\">stationary</a>, which is often required for analysis. In our case, the time series is not stationary which may require some differencing, and transformation operations.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Some linear time series forecasting methods assume a well-behaved distribution of observations (i.e. a bell curve or normal distribution). This can be explicitly checked using tools like statistical hypothesis tests. But plots can provide a useful first check of the distribution of observations both on raw observations and after any type of data transform has been performed. Concerning the histogram and kde plots, the curves are typical of an exponential decay.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the series against time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We are seeing drastic changes over time. \n",
    "#We will plot the log of the time series against time to get a better reality of the evolution. \n",
    "#Use loglog=True as an argument\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# density plot\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Another type of plot that is useful to summarize the distribution of observations is the box and whisker plot. \n",
    "    \n",
    "This plot draws a box around the 25th and 75th percentiles of the data that captures the middle 50% of observations. A line is drawn at the 50th percentile (the median) and whiskers are drawn above and below the box to summarize the general extents of the observations. Dots are drawn for outliers outside the whiskers or extents of the data. \n",
    "    \n",
    "    \n",
    "From the whiskers plot below, we can see the disparity in data with years. The first 3 years have much smaller values, and are almost insignificant compared to the values in 2017 and 2018. In some analysis, the first years would be considered as outliers, however here as the data available is not enough it is important to keep these years. Also, as seen on the log-log plot of the time series, some recurrent behaviour is seen, such as cycles with a big increase of the coin value in 2017 as explained above.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Box and Whisker plots. SPEND SOME TIME UNDERSTANDING THIS CODE\n",
    "groups = series['2013':'2018'].groupby(pd.Grouper(freq='Y'))\n",
    "\n",
    "years_df = list()\n",
    "\n",
    "for name, group in groups:\n",
    "    df = pd.DataFrame()\n",
    "    df[name.year] = group.values\n",
    "    years_df.append(df)\n",
    "# concatenate the columns of the different coins and save the dataset to a csv file\n",
    "years = pd.concat(years_df, axis = 1, sort=False)\n",
    "\n",
    "years.boxplot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autocorrelation Analysis\n",
    "\n",
    "<p style=\"text-align: justify\">Statistical correlation summarizes the strength of the relationship between two variables.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can assume the distribution of each variable fits a <a href=\"https://en.wikipedia.org/wiki/Normal_distribution\">Gaussian</a> (bell curve) distribution. If this is the case, we can use the Pearson’s correlation coefficient to summarize the correlation between the variables.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The Pearson’s correlation coefficient is a number between -1 and 1 that describes a negative or positive correlation respectively. A value of zero indicates no correlation.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can calculate the correlation for time series observations with observations with previous time steps, called lags. Because the correlation of the time series observations is calculated with values of the same series at previous times, this is called a serial correlation, or an autocorrelation.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">A plot of the autocorrelation of a time series by lag is called the <a href=\"https://machinelearningmastery.com/gentle-introduction-autocorrelation-partial-autocorrelation/\">AutoCorrelation Function</a>, or the acronym ACF. This plot is sometimes called a <a href=\"https://en.wikipedia.org/wiki/Correlogram\">correlogram</a>, or an autocorrelation plot.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">A partial autocorrelation function or PACF is a summary of the relationship between an observation in a time series with observations at prior time steps with the relationships of intervening observations removed.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The autocorrelation for an observation and an observation at a prior time step is comprised of both the direct correlation and indirect correlations. These indirect correlations are a linear function of the correlation of the observation, with observations at intervening time steps.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">It is these indirect correlations that the partial autocorrelation function seeks to remove. Without going into the math, this is the intuition for the partial autocorrelation.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can calculate autocorrelation and partial autocorrelation plots using the <a href=\"http://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_acf.html\">plot_acf()</a> and <a href=\"http://www.statsmodels.org/dev/generated/statsmodels.graphics.tsaplots.plot_pacf.html\">plot_pacf()</a> statsmodels functions respectively.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can then create a single figure that contains both an ACF and a PACF plot. The number of lag time steps can be specified. We will fix this to be 100 days for the autocorrelation, and 50 days for the partial autocorrelation. We would expect that the price of the currency tomorrow, and in the coming week will be dependent upon the price from prior days. As such, we would expect to see a strong autocorrelation signal in the ACF and PACF plots.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot Autocorrelation and Partial ACF plots with 100 and 50 lags respectively\n",
    "fig, (ax1, ax2) = plt.subplots(nrows=2, ncols=1, figsize = (8,8))\n",
    "\n",
    "plot_acf() \n",
    "plot_pacf()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<p style=\"text-align: justify\">We can clearly see a familiar autoregression pattern across the two plots. This pattern is comprised of two elements:</p>\n",
    "\n",
    "<ul>\n",
    "<li><strong>ACF</strong>: A large number of significant lag observations that slowly degrade as the lag increases.</li>\n",
    "<li><strong>PACF</strong>: A few significant lag observations that abruptly drop as the lag increases.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-align: justify\">The ACF plot indicates that there is a strong autocorrelation component, whereas the PACF plot indicates that this component is distinct for the first approximately two lag observations. However, most of the lags are out of the interval of confidence, and it is difficult to characterize the current plot.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented Dickey-Fuller test\n",
    "<p style=\"text-align: justify\">Statistical tests make strong assumptions about your data. They can only be used to inform the degree to which a null hypothesis can be rejected or fail to be reject. The result must be interpreted for a given problem to be meaningful.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Nevertheless, they can provide a quick check and confirmatory evidence that your time series is stationary or non-stationary.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The <a href=\"https://en.wikipedia.org/wiki/Augmented_Dickey%E2%80%93Fuller_test\">Augmented Dickey-Fuller test</a> is a type of statistical test called a <a href=\"https://en.wikipedia.org/wiki/Unit_root_test\">unit root test</a>.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The intuition behind a unit root test is that it determines how strongly a time series is defined by a trend.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">There are a number of unit root tests and the Augmented Dickey-Fuller may be one of the more widely used. It uses an autoregressive model and optimizes an information criterion across multiple different lag values.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The null hypothesis of the test is that the time series can be represented by a unit root, that it is not stationary (has some time-dependent structure). The alternate hypothesis (rejecting the null hypothesis) is that the time series is stationary.</p>\n",
    "\n",
    "<ul>\n",
    "<li><strong>Null Hypothesis (H0)</strong>: If failed to be rejected, it suggests the time series has a unit root, meaning it is non-stationary. It has some time dependent structure.</li>\n",
    "<li><strong>Alternate Hypothesis (H1)</strong>: The null hypothesis is rejected; it suggests the time series does not have a unit root, meaning it is stationary. It does not have time-dependent structure.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-align: justify\">We interpret this result using the p-value from the test. A p-value below a threshold (such as 5% or 1%) suggests we reject the null hypothesis (stationary), otherwise a p-value above the threshold suggests we fail to reject the null hypothesis (non-stationary).</p>\n",
    "<ul>\n",
    "<li><strong>p-value &gt; 0.05</strong>: Fail to reject the null hypothesis (H0), the data has a unit root and is non-stationary.</li>\n",
    "<li><strong>p-value &lt;= 0.05</strong>: Reject the null hypothesis (H0), the data does not have a unit root and is stationary.</li>\n",
    "</ul>\n",
    "\n",
    "<p style=\"text-align: justify\">Below is an example of calculating the Augmented Dickey-Fuller test on the current dataset. The statsmodels library provides the <a href=\"http://statsmodels.sourceforge.net/devel/generated/statsmodels.tsa.stattools.adfuller.html\">adfuller()</a> function that implements the test.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if stationary\n",
    "result =  #ADF test\n",
    "\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %f' % result[1])\n",
    "print('Critical Values:')\n",
    "\n",
    "for key, value in result[4].items():\n",
    "\tprint('\\t%s: %.3f' % (key, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Running the example outputs the result of a statistical significance test of whether the series is stationary. Specifically, the augmented Dickey-Fuller test.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The results show that the test statistic value -3.123 is smaller than the critical value at 5% of -2.863. This suggests that we can reject the null hypothesis with a significance level of less than 5% (i.e. a low probability that the result is a statistical fluke).</p>\n",
    "\n",
    "Rejecting the null hypothesis means that the process has no unit root, and in turn that the **time series is stationary or does not have time-dependent structure**. Interestingly, the current time series can be considered stationary, when it doesn't seem to be so. This implies that no differencing operation will be required for analysis with the different models.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Additionally, if we want to have more information about the decomposition of the time series (since we now know that it is stationary), we can try the <b>tsa.seasonal_decompose()</b> function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "# Frequence discussion reference: https://stats.stackexchange.com/questions/285718/seasonal-decomposition\n",
    "sm.tsa.seasonal_decompose(, freq=500).plot(); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***NOTE:*** It might be more intuitive to do the **_ADF Test_** and the **_Autocorrelation Analysis_** in another order (e.g., check the ADF Test first and then the Autocorrelation Analysis), since we might have a case of **p-value > 0.05**, where doing any further stationary analysis is pointless. In this specific example we knew beforehand that it was going to work well, but it might not be the case on real data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation metric\n",
    "\n",
    "<p style=\"text-align: justify\">A forecast will be comprised of seven values, one for each day of the week ahead.</p>\n",
    "<p style=\"text-align: justify\">It is common with multi-step forecasting problems to evaluate each forecasted time step separately. This is helpful for a few reasons:</p>\n",
    "<ul>\n",
    "<li>To comment on the skill at a specific lead time (e.g. +1 day vs +3 days).</li>\n",
    "<li>To contrast models based on their skills at different lead times (e.g. models good at +1 day vs models good at days +5).</li>\n",
    "</ul>\n",
    "<p style=\"text-align: justify\">Both <a href=\"https://en.wikipedia.org/wiki/Root-mean-square_deviation\">Root Mean Squared Error (RMSE)</a> and <a href=\"https://en.wikipedia.org/wiki/Mean_absolute_error\">Mean Absolute Error (MAE)</a> can be used to quantify the errors in price prediction (USD), although RMSE is more commonly used and will be adopted in this notebook. Unlike MAE, RMSE is more punishing of forecast errors.</p>\n",
    "<p style=\"text-align: justify\">The performance metric for this problem will be the RMSE for each lead time from day 1 to day 7. In this way, we can see how the chosen algorithms perform on the predictions at a particular day of the week. The cryptocurrency market is quite volatile, and may have a different behaviour depending on the period of the week (weekdays or weekend for instance).</p>\n",
    "<p style=\"text-align: justify\">As a short-cut, it may be useful to summarize the performance of a model using a single score in order to help in model selection. One possible score that could be used would be the RMSE across all forecast days.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The function <i>evaluate_forecasts()</i> below will implement this behavior and return the performance of a model based on multiple seven-day forecasts.</p>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate one or more weekly forecasts against expected values. SPEND SOME TIME UNDERSTANDING THIS CODE\n",
    "def evaluate_forecasts(actual, predicted):\n",
    "    \n",
    "    scores = list()\n",
    "    \n",
    "    # calculate an RMSE score for each day\n",
    "    \n",
    "    for i in range(actual.shape[1]):\n",
    "        # calculate mse\n",
    "        mse = mean_squared_error(actual[:, i], predicted[:, i])\n",
    "        # calculate rmse\n",
    "        rmse = sqrt(mse)\n",
    "        # store\n",
    "        scores.append(rmse)\n",
    "    \n",
    "    # calculate overall RMSE\n",
    "    \n",
    "    s = 0\n",
    "    \n",
    "    for row in range(actual.shape[0]):\n",
    "    \n",
    "        for col in range(actual.shape[1]):\n",
    "            s += (actual[row, col] - predicted[row, col])**2\n",
    "    \n",
    "    score = sqrt(s / (actual.shape[0] * actual.shape[1]))\n",
    "    \n",
    "    return score, scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Running the function will first return the overall RMSE regardless of day, then an array of RMSE scores for each day.</p>\n",
    "\n",
    "## Train and Test Sets\n",
    "\n",
    "<p style=\"text-align: justify\">We will use the first 4 years of data for training predictive models and the final 2 years for evaluating models.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The data in a given dataset will be divided into standard weeks. These are weeks that begin on a Monday and end on a Sunday.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">This is a realistic and useful way for using the chosen framing of the model, where the price for the week ahead can be predicted. It is also helpful with modeling, where models can be used to predict a specific day (e.g. Wednesday) or the entire sequence.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We will split the data into standard weeks, working backwards from the test dataset. This gives 178 weeks of data for the training set and 102 weeks (714 days) for the testing set.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The function <em>split_dataset()</em> below splits the daily data into train and test sets and organizes each into standard weeks. The \"<i>n_test</i>\" argument corresponds to the number of days (714 in this study), to cut the data backwards.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">Specific row offsets are used to split the data using knowledge of the dataset. The split datasets are then organized into weekly data using the NumPy <a href=\"https://docs.scipy.org/doc/numpy/reference/generated/numpy.split.html\">split() function</a>.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split a univariate dataset into train/test sets\n",
    "def split_dataset(data, n_test):\n",
    "    # split into standard weeks\n",
    "    train, test = data[0:-n_test], data[-n_test:]\n",
    "    # restructure into windows of weekly data\n",
    "    train = np.array(np.split(train, len(train)/7))\n",
    "    test = np.array(np.split(test, len(test)/7))\n",
    "    return train, test\n",
    "\n",
    "# testing the procedure on the current data\n",
    "n_test =  # number of days to split the dataset in weekly windows\n",
    "train, test = split_dataset(values, n_test)\n",
    "# validate train data. How many data points are in the training serie?\n",
    "\n",
    "# validate test. How many data points are in the test serie?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walk-forward validation\n",
    "\n",
    "<p style=\"text-align: justify\">Models will be evaluated using a scheme called <a href=\"https://en.wikipedia.org/wiki/Walk_forward_optimization\">walk-forward validation</a>.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">This is where a model is required to make a one week prediction, then the actual data for that week is made available to the model so that it can be used as the basis for making a prediction on the subsequent week. This is both realistic for how the model may be used in practice and beneficial to the models allowing them to make use of the best available data.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We can demonstrate this below with separation of input data and output/predicted data.</p>\n",
    "\n",
    "    Input, \t\t\t\t\t\tPredict\n",
    "    [Week1]\t\t\t\t\t\tWeek2\n",
    "    [Week1 + Week2]\t\t\t\tWeek3\n",
    "    [Week1 + Week2 + Week3]\t\tWeek4\n",
    "\n",
    "<p style=\"text-align: justify\">The walk-forward validation approach to evaluating predictive models on this dataset is implement below, named <em>evaluate_model()</em>.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The name of a function is provided for the model as the argument \"<em>model_func</em>\";. This function is responsible for defining the model, fitting the model on the training data, and making a one-week forecast.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The forecasts made by the model are then evaluated against the test dataset using the previously defined <em>evaluate_forecasts()</em> function.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SPEND SOME TIME UNDERSTANDING WHAT'S HAPPENING HERE.\n",
    "def evaluate_model(model_func, train, test, *args):\n",
    "    #history of weekly data\n",
    "    history = [x for x in train]\n",
    "    #walk forward validation\n",
    "    predictions = list()\n",
    "    for i in range(len(test)):\n",
    "    #weekly prediction\n",
    "        y_hat_seq = model_func(history, *args)\n",
    "    #store the preditions\n",
    "        predictions.append(y_hat_seq)\n",
    "    #update history data\n",
    "        history.append(test[i,:])\n",
    "    predictions = np.array(predictions)\n",
    "    # evaluate predictions days for each week\n",
    "    score, scores = evaluate_forecasts(test[:, :, 0], predictions)\n",
    "    return score, scores, predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">Once we have the evaluation for a model, we can summarize the performance.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The function below named <em>summarize_scores()</em> will display the performance of a model as a single line for easy comparison with other models.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Summarize scores\n",
    "def summarize_scores(name, score, scores):\n",
    "    s_scores = ', '.join(['%.1f' % s for s in scores])\n",
    "    print('%s: Total RMSE --> [%.3f]; Daily RMSE: %s' % (name, score, s_scores))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop an Autoregression Model\n",
    "\n",
    "<p style=\"text-align: justify\">The <em>to_series()</em> function below will take the multivariate data divided into weekly windows and will return a single univariate time series.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert windows of weekly multivariate data into a series of closing price\n",
    "def to_series(data):\n",
    "    # extract just the price of XRP from each week\n",
    "    series = [week[:, 0] for week in data]\n",
    "    # flatten into a single series\n",
    "    series = np.array(series).flatten()\n",
    "    return series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">We can develop an autoregression model for univariate series of XRP price.\n",
    "\n",
    "<p style=\"text-align: justify\">The Statsmodels library provides multiple ways of developing an AR model, such as using the AR, ARMA, ARIMA, and SARIMAX classes.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">We will use the <a href=\"http://www.statsmodels.org/dev/generated/statsmodels.tsa.arima_model.ARIMA.html\">ARIMA implementation</a> as it allows for easy expandability into differencing and moving average. </p>\n",
    "\n",
    "<p style=\"text-align: justify\">The <em>arima_forecast()</em> function defined below implements the procedure for doing a prediction with an ARIMA model. First, the history data comprised of weeks of prior observations must be converted into a univariate time series of daily closing price. We can use the <em>to_series()</em> function developed in the previous section.</p>\n",
    "\n",
    "```python\n",
    "# convert history into a univariate series\n",
    "series = to_series(history)\n",
    "```\n",
    "<p style=\"text-align: justify\">Next, an ARIMA model can be defined by passing order arguments (\"<em>arima_order</em>\") to the constructor of the ARIMA class. And the model can be fit on the training data. We will use the defaults and disable all debugging information during the fit by setting <em>disp=False</em>.</p>\n",
    "\n",
    "```python\n",
    "# define the model\n",
    "model = ARIMA(series, order=arima_order)\n",
    "# fit the model\n",
    "model_fit = model.fit(disp=False)\n",
    "```\n",
    "\n",
    "<p style=\"text-align: justify\">Now that the model has been fit, we can make a prediction.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">A prediction can be made by calling the <em>predict()</em> function and passing it either an interval of dates or indices relative to the training data. We will use indices starting with the first time step beyond the training data and extending it six more days, giving a total of a seven day forecast period beyond the training dataset.</p>\n",
    "\n",
    "```python\n",
    "# make forecast\n",
    "yhat = model_fit.predict(len(series), len(series)+6)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arima forecast for weekly prediction\n",
    "def arima_forecast(history, arima_order):\n",
    "    # convert history into a univariate series\n",
    "\n",
    "    # define the model\n",
    "\n",
    "    # fit the model\n",
    "\n",
    "    # make forecast\n",
    "\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The seasonal ARIMA model <a href=\"https://www.statsmodels.org/dev/generated/statsmodels.tsa.statespace.sarimax.SARIMAX.html\">SARIMA</a>, can also be implemented in a similar way as the ARIMA model. The <em>Sarima_forecast</em> function below implements the same procedure as above, with the <em>\"config\"</em> argument defining the configuration of the chosen SARIMA model. The seasonal ARIMA model takes more parameters than the regular ARIMA model, to characterize some seasonal trends that might be present inside the dataset.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sarima forecast for weekly prediction\n",
    "def Sarima_forecast(history, config):\n",
    "    order, sorder, trend = config\n",
    "    # convert history into a univariate series\n",
    "    series = to_series(history)\n",
    "    # define model\n",
    "    model = SARIMAX(series, order=order, seasonal_order=sorder, trend=trend, enforce_stationarity=False, enforce_invertibility=False)\n",
    "    # fit model\n",
    "    model_fit = model.fit(disp=False)\n",
    "    # make one step forecast\n",
    "    yhat = model_fit.predict(len(series), len(series)+6)\n",
    "    return yhat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A [grid search with hyperparameters](https://machinelearningmastery.com/grid-search-arima-hyperparameters-with-python/) has been carried out to find the most suitable ARIMA and SARIMA models for the current study. We find that <b>ARIMA(1,0,0)</b> and <b>SARIMA((0,1,0),(0,0,0,0),'n')</b> give the best results out of the different parameters tested, in this configuration.</p> \n",
    "\n",
    "<p style=\"text-align: justify\">A dictionary containing both models is then created, to store the names of the models, which will be useful for future looping functions. Both orders are defined as <em>\"arima_order\"</em> for ARIMA, and <em>\"config_sarima\"</em> for the SARIMA model. Both parameters are stored in a list <em>\"orders\"</em> that will be called in the evaluation of the algorithms.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the names and functions for the models we wish to evaluate\n",
    "models = dict()\n",
    "models['arima'] = \n",
    "models['Sarima'] = \n",
    "\n",
    "# define both parameters for the 2 models. We mentioned which are the most suitable ones before\n",
    "arima_order = \n",
    "config_sarima = \n",
    "\n",
    "# list creation to store the parameters\n",
    "orders = list()\n",
    "orders.extend([arima_order, config_sarima])\n",
    "\n",
    "# list containing the days of the week for plot on a particular day\n",
    "days = ['mon', 'tue', 'wed', 'thr', 'fri', 'sat', 'sun']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">After defining the different functions, it is now possible to evaluate the different algorithms, summarizing their scores and plotting the RMSE for each day of the week. The function <em>evaluate_algorithms()</em> below implements this procedure by looping over the dictionary of models, and saving the predictions of each model in a list of results, to be used for plots afterwards.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate each model\n",
    "def evaluate_algorithms(models, orders):\n",
    "    prediction_simulations = list()\n",
    "    for (name, func), order in zip(models.items(), orders): # loop on dictionary items and list orders at same time\n",
    "        # evaluate and get scores\n",
    "        score, scores, predictions = evaluate_model(func, train, test, order)\n",
    "        # summarize scores\n",
    "        summarize_scores(name, score, scores)\n",
    "        # plot scores\n",
    "        plt.plot(days, scores, marker='o', label=name)\n",
    "        # append the simulation results in a list\n",
    "        prediction_simulations.append(predictions)\n",
    "    # plot properties and showing\n",
    "    plt.ylabel('RMSE')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    return prediction_simulations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The function <em>plot_forecasts()</em> creates a plot showing the evolution of the cryptocurrency price for the testing set, and the predictions made by ARIMA and SARIMA models.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">It is also interesting to look at the residuals between prediction and testing data. A dictionary <em>\"residuals_model\"</em> is first created to store the dataframes that will contain the residuals for the models, with their names as keys. The function <em>plot_residuals()</em> is the implemented to create a figure with 3 subplots, showing from top to bottom:</p>\n",
    "\n",
    "<ul>\n",
    "    <li>Evolution of residuals with time for both models.</li>\n",
    "    <li>Histogram of the distributions of the residuals</li>\n",
    "    <li>Kernel density estimation of the residuals distributions</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the forecasts in the context of the original dataset\n",
    "def plot_forecasts(models, train, test, prediction_simulations, colors ,n_test):\n",
    "    # reshape train and tests set --> flattened for each feature\n",
    "    train = np.reshape(train, (train.shape[0]*train.shape[1],train.shape[2]))\n",
    "    test = np.reshape(test, (test.shape[0]*test.shape[1],test.shape[2]))\n",
    "    \n",
    "    #plt.plot(train) # plot training dataset\n",
    "    plt.plot([None for i in train] + [x for x in test[:,0]], color = 'green', label=\"testing set\")\n",
    "    \n",
    "    for i, (name, model), color in zip(range(len(prediction_simulations)), models.items(), colors): # looping over all results and arguments together \n",
    "        predictions = np.array(prediction_simulations[i]).flatten() # flatten predictions\n",
    "        plt.plot([None for i in train] + [i for i in predictions] , color=color, label=name)\n",
    "     \n",
    "    # define different properties on the plot\n",
    "    plt.xlabel('Days')\n",
    "    plt.ylabel('Price ($)')\n",
    "    plt.suptitle(\"Evolution of XRP price with time\",fontsize=18, fontweight='bold')\n",
    "    plt.legend()\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    residuals_model = dict() # initialize dictionary to store dataframes of residuals\n",
    "\n",
    "\n",
    "def plot_residuals(models, test, prediction_simulations, colors,n_test):\n",
    "    \n",
    "    \n",
    "    \n",
    "    # redefine train and test set without the weekly reshaping\n",
    "    test = np.reshape(test, (test.shape[0]*test.shape[1],test.shape[2])) # flatten test matrix\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(nrows=3, ncols=1, figsize = (8,8)) # figure creation with 2 subplots\n",
    "    \n",
    "    for i, (name, model), color in zip(range(len(prediction_simulations)), models.items(), colors): # looping over all results and arguments together\n",
    "        predictions = np.array(prediction_simulations[i]).flatten() # flatten predictions\n",
    "        residuals = [test[i,0] - predictions[i] for i in range(len(test))] # list comprehension to calculate residuals\n",
    "        residuals = pd.DataFrame(residuals) # creates a dataframe\n",
    "        residuals.plot(kind = 'line', ax = ax1, color=color) # line plot\n",
    "        residuals.hist(ax = ax2, bins = 30, color=color) # histogram\n",
    "        residuals.plot(kind = 'kde', ax = ax3, color=color) # kernel density estimation\n",
    "        \n",
    "        residuals_model[name] = residuals # store the residuals dataframe inside the dictionary\n",
    "        \n",
    "    # define different properties on the plot\n",
    "    ax1.set_ylabel('Residual value ($)')\n",
    "    ax1.set_xlabel('Time (days)')\n",
    "    ax2.set_ylabel('Counts')\n",
    "    plt.xlabel('Price ($)')\n",
    "    ax1.legend(['arima','Sarima'])\n",
    "    ax2.legend(['arima','Sarima'])\n",
    "    ax3.legend(['arima','Sarima'])\n",
    "    fig.suptitle(r'Evolution of the residuals', fontsize=18, fontweight='bold', y=0.92)\n",
    "    # show the plot\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The script below defines all the parameters necessary to run the simulation, split the dataset into a training and testing dataset, and call the <em>evaluate_algorithms()</em> function to make predictions on the testing dataset with ARIMA and SARIMA models.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_test =  # Splitting the dataset 714 days before the end\n",
    "\n",
    "#n_tests = [714, 350, 1400]\n",
    "# define the names and functions for the models we wish to evaluate\n",
    "models = dict()\n",
    "models['arima'] = arima_forecast\n",
    "models['Sarima'] = Sarima_forecast\n",
    "\n",
    "arima_order = (1,0,0)\n",
    "config_sarima = (0,1,0),(0,0,0,0),'n'\n",
    "\n",
    "orders = list()\n",
    "orders.extend([arima_order, config_sarima])\n",
    "\n",
    "\n",
    "# Creating a list of colors to be passed as arguments for the plots\n",
    "colors = list()\n",
    "colors.extend(['red','blue'])\n",
    "\n",
    "#for n_test in n_tests:\n",
    "train, test = split_dataset(dataset.values, n_test)\n",
    "\n",
    "# evaluate the different algorithms defined in \"models\" dictionary\n",
    "prediction_simulations = evaluate_algorithms(models, orders)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">As seen on the figure above, and the printed scores, the ARIMA and SARIMA models performs similarly. This is expected, considering that we are using rolling windows of 7 days in which, a seasonal pattern should hardly exist. Considering the training and testing dataset as a whole for instance would lead to different results as seasonal patterns do exist, especially when considering cryptocurrencies or other financial assets.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The RMSE error also increases as the week goes on. Indeed, considering the models are autoregressive with a lag of 1, the error gradually increases through the week depending on the evolution of the price compared to monday.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">The evolution of price with time is then plotted for the predictions and the testing dataset. It can be seen why the error increases with the AR(1) model, especially when considering a cryptocurrency such as XRP, which is quite volatile. For the ARIMA model, the weekly prediction is based on the value from the previous week, with some seasonal moving average added in the SARIMA model. This shows the limitations of the models here despite the hyper-grid search carried out previously.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_forecasts(models, train, test, prediction_simulations, colors ,n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_residuals(models, test, prediction_simulations, colors,n_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"text-align: justify\">The residuals evolution and statistics, shows a Gaussian distribution which located close to a mean of 0, which is something we need to achieve while considering the residuals of a prediction.</p>\n",
    "\n",
    "<p style=\"text-align: justify\">In the next cell, different statistics of the residuals are printed for both models. We can see, that despite having a mean close to 0, the standard deviation for both models is too high in comparison of the values at stake. One main reason is due to the big jump in XRP price after 1600 days (in 2017), which created some disruption in the models with some high residual values. A glance at the percentiles for instance for the ARIMA model, shows that 75% of the values are predicted with less than 0.02\\$ difference, and even better for the SARIMA with the values forecasted with less than 0.013\\$ on the same range, which is good already considering the price value of the currency on that period.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Describe both the ARIMA AND SARIMA stats\n",
    "\n",
    "print('ARIMA stats :', )\n",
    "print()\n",
    "print('SARIMA stats :', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions\n",
    "\n",
    "<p style=\"text-align: justify\">This section lists some ideas for extending this example that you may wish to explore.</p>\n",
    "\n",
    "<ul>\n",
    "\n",
    "<li><div style=\"text-align: justify\"><strong>Change the walk-forward validation procedure</strong>. Here, we used a rolling window of 7 days to predict the next 7 days of the cryptocurrency price. Depending on the problem encountered, there might be an optimal window in which the models need to be trained and re-defined for improved results.</div></li>\n",
    "\n",
    "<li><div style=\"text-align: justify\"><strong>Correct predictions with residual errors</strong>. A model of forecast residual error is interesting, but it can also be useful to make better predictions. With a good estimate of forecast error at a time step, we can make better predictions. For example, we can add the expected forecast error to a prediction to correct it and in turn improve the skill of the model.</div></li>\n",
    "    \n",
    "<li><strong>Explore Data Preparation</strong>. The model was fit on the raw data directly. Explore whether standardization or normalization or even power transforms can further improve the skill of the AR model.</li>\n",
    "\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
