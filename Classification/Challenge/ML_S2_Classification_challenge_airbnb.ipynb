{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **AISaturdays Rental Challenge**\n",
    "\n",
    "<img src=\"https://do3z7e6uuakno.cloudfront.net/uploads/event/logo/1112702/595053a7143adafce285b2e39ca04f1a.jpeg\" width=\"300\">\n",
    "\n",
    "**Instructions:**\n",
    "\n",
    "- You'll be using Python 3.\n",
    "- You'll use Python's libraries: Pandas, MatPlotLib, Numpy.\n",
    "\n",
    "**Completing the exercise, you¬¥ll learn to:**\n",
    "- Better use and understand Python NoteBooks.\n",
    "- Be able to use python functions and additional libraries.\n",
    "- Dataset:\n",
    " - Obtain the dataset and visualize the information contained.\n",
    " - Clean and normalise the dataset's information.\n",
    " - Represent and analyse the dataset's information.\n",
    "- Correctly apply the Random Forest Algorithm.\n",
    "- Improve the predictions using Hyperparameter Tunning, Feature engineering and Gradient Boosting\n",
    "\n",
    "¬°Let's get started!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#1.Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import statsmodels.api as sm\n",
    "import scipy.stats as stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#2. Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open the .csv ('AB_NY_2019.csv') and display the first few lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Show the number of features and examples in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Obtain the (dtypes) of the features and the number of rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables\n",
    "\n",
    "\n",
    "\n",
    "* **Id/name:**  Identifier and name of the property.\n",
    "\n",
    "* **host_id/host_name:** Identifier and host name.\n",
    "\n",
    "* **neighbourhood_group/neighbourhood:** Area and neighbourhood of the property. Each Area is a group of neighbourhoods.\n",
    "* **latitude/longitude:** Latitude and longitude of the property.\n",
    "\n",
    "* **room_type:** The type of property offered. It can be a single room, a shared apartment or a whole property.\n",
    "\n",
    "* **minimum_nights:**  Minimum number of nights to stay.\n",
    "\n",
    "* **number_of_reviews:**  Total number of reviews.\n",
    "\n",
    "* **last_review:**  Date of the last review.\n",
    "\n",
    "* **reviews_per_month:** Monthly number of reviews. It's rarely *int* and it can be less than one..\n",
    "\n",
    "* **calculated_host_listings_count:** Total number of properties the host is offering.\n",
    "\n",
    "* **availability_365:** Yearly availability of the flat in number of days: maximum is 365 days ( the whole year).\n",
    "\n",
    "* **price:** Our target variable! The price of the proeprty in dollars.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this truly a regression or classification problem? Why?:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Before analyzing the dataset, we need to transform the dates (last_review) into something with which we can work. Pandas has a specific format for this, datetime. Change last_review to datetime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. To analyse the data we also need to lnow how much information we're missing. Use .isnull() to find out which feature is missing more values. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Finally, we need to drop the features that are only identifiers and are not useful for predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. All ready! We can analyse the distribution of data with .describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning and normalization of the dataset\n",
    "![texto alternativo](https://i.imgur.com/8u4xTI7.png)\n",
    "\n",
    "This dataset contains incomplete information that we need to impute to be able to use it for the rpediction of the property prices.\n",
    "We also need to transform *last_review* if we eant to use it for the prediction, we cannot use it directly as a date.\n",
    "For this processing we'll use Pandas functions, you already have a cheatsheet from the previous session, but take a look at this other [cheatsheet](https://assets.datacamp.com/blog_assets/PandasPythonForDataScience.pdf)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Find the number of columns that do not have reviews and hence have empty values for last_review and reviews_per_month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. We need to complete this information if we don't want to disregard the rest of the features. Fill in the NaNs of reviews_per_month with 0 (We'll take last_review later on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. Time to transform the variable *last_review*. It's a date, which makes it hard to work with. Let's first complete the examples that do not have a last date. Replace these NaNs with the first historical revision of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. Now that we don't have empty values we can change the last_review variable to something more useful. We look for smaller values to correspond to old or no reviews, while larger values correspond to more recent reviews.\n",
    "We can use the toordinal() function to find the number of days that have elapsed since day 1 of year 1, but those are still too large numbers that don't follow the distribution we're looking for.\n",
    "\n",
    "Gets last_reviews to represent the number of days that have elapsed since the first historical review was done. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. To visualize the distribution of dates, generate a graph showing the variable last_reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It seems there are two well distinguished groups. What do you think is it causing this?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study of the variable to predict and noise elimination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. A la hora de predecir el precio, es mucho mas favorable si primero transformamos y analizamos la variable que buscamos para hacerla mas facil de predecir.\n",
    "12. When it comes to predicting the price, it is much more favorable if we first transform and analyze the variable we are looking for to make it easier to predict.\n",
    "\n",
    "First, let's see how the price of the offers is distributed. Generate a graph showing the price of the bids. Here's a [hint.](https://seaborn.pydata.org/generated/seaborn.distplot.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have a variable that follows a log-normal distribution. We can transform it into a normal distribution by applying log1p(), a function that responds to the following equation:\n",
    "\n",
    "ùë¶=ùëôùëúùëî(ùë•+1)\n",
    "\n",
    "This makes the price easier to predict, as it has a normal distribution.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. Let's visualize this transformation. Generates another price graph after applying the log1p() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have a much more appropriate distribution to make predictions. However, there are still many outliers that add noise to the sample."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. Above and below what values is this noise present? Eliminates from the dataframe those values that do not fit into the normal distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15. Now, rebuild the price plot and price log1p (use the same code as before, or put it in a [subplot](https://matplotlib.org/api/_as_gen/matplotlib.pyplot.subplots.html ))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "16. Finally, we have a noise-free normalized output variable that will improve our predictions. Change the variable price to the log1p of price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to explore a little more the rest of the variables that can affect the price of an offer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "17. Let's start by creating a histogram of the different areas of the city and the number of offers in each of them (you may need to enlarge the graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "18. Now create a map of the offered apartments with latitude and longitude (extra points if you color them by areas or neighborhoods). It's best to do it in a subplot so you can control the size of the map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "19. We are now going to generate another histogram, this time with the type of room offered (It is also a good idea to adjust the size of the graph)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Variable Transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply the same process that we applied to the price variable to our input variables and thus achieve a more comfortable distribution for the search methods.\n",
    "\n",
    "Apply the log1p() transformation to minimum_nights, generating the before and after plots and compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Finally, save minimum_nights as log1p of minimum_nights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-liner code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "21. Repeat the process, this time with reviews_per_month. Is transformation relevant?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Study of availability in number of days (0.365)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "22. Let's start by representing the availability in a distplot(). Since we know the limits of this variable, it is best to limit the range of the graph and make it larger.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Four lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add artificial variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It has been seen in the scatterplot above that there seem to be two groups, one available most of the year and another only a few days.\n",
    "\n",
    "It is also intuited that those sites that do not have reviews... How do they not give much confidence? ;)\n",
    "\n",
    "Add three categories that measure if the apartment is available all year round, if its availability is very low (less than 12 days a year), and if it has no reviews."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "23. Add three categories that measure if the apartment is available all year round, if its availability is very low (less than 12 days a year), and if it has no reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "24. We are going to generate a heatmap that shows the relationship between all the input variables and price. It uses corr() and seaborn's heatmap() function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pass categorical variables to one_hot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "25. To make the categorical features easier to interpret by the model, we are going to transform them into a OneHotEncoding. Use pandas get_dummies() function (you should have 241 columns left)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models, models, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all the exploration, analysis and data cleaning done, we move on to the fun part: The models!\n",
    "\n",
    "We start by importing all the classes that we are going to need to find a good predictive model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split,cross_val_score,  GridSearchCV, KFold, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "26. Split the dataset into X_train, X_test, y_train and y_test using train_test_split(). Don't forget not to include price in the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "27. We are going to use cross_validation to train our model, using Kfold to find the score. Implement a Kfold that performs 5 splits and calculates the mean error and deviation of a RandomForestRegressor without changing its parameters (yet). [Hint](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "28. When using a RandomForestRegressor, what hyperparameters were we using? List all the parameters used by this model (it uses the get_params() function and the pprint library)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "## Two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can adjust all of these parameters to improve the accuracy of our model. One way to find which combination works best is to use a GridSearchCV, which tests models with many different combinations and calculates their score to find the best brute force model. For this, you have to pass a list of values for each parameter, and GridSearchCV will try all of them. [More information](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "29. Delimit what values you want each parameter to have, and include each of these lists in a dictionary to be able to execute the GridSearchCV. Note the possible values for each of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eight lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30. Now we can implement a GridSearchCV. To make it faster, a version is used that does not test all possible combinations, but only a few random ones (hence its name, RandomizedSearchCV). Implement it, taking into account that it has as parameters the model to be adjusted and the dictionary that we have defined before, among others. This step may take a few minutes as you have to adjust many models to find the best one. Here is the [documentation](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) for the RandomizedSearchCV."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Two lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "31. Finally, find the mean squared error and ùëÖ2 of the best model you have created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Six lines of code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, to improve that score! You can try:\n",
    "\n",
    "- Remove features that are not relevant to the prediction\n",
    "- Implement Gradient boosting using XBoost or Adaboost, among others\n",
    "- Tune hyperparameters manually to arrive at better models\n",
    "- Use a Tree Interpreter to see which decision trees are most important\n",
    "\n",
    "Let's see who achieves the best score! üöÄ"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
